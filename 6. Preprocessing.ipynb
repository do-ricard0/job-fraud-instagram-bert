{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Sastrawi\n",
      "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Sastrawi\n",
      "Successfully installed Sastrawi-1.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01aa974a-a9f1-4990-909a-a6ed737e2fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Menghilangkan karakter non-alfanumerik dan mengganti beberapa karakter\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Opsi tambahan: mengganti newline dengan spasi, dll.\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba99a4af-d5aa-4667-9c53-82ac608aa528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inisialisasi Tokenizer mBERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "971e9cd1-cf88-4ca5-9b0b-5b1df40d3d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sliding_window_tokenize(text, max_length=700, stride=50):\n",
    "    # Konversi teks ke token dan inisialisasi segmen\n",
    "    initial_tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    total_tokens = len(initial_tokens)\n",
    "    window_segments = []\n",
    "\n",
    "    if total_tokens <= max_length:\n",
    "        # Jika total token kurang dari max_length, tidak perlu sliding window\n",
    "        return [tokenizer.encode(text, add_special_tokens=True)]\n",
    "\n",
    "    for i in range(0, total_tokens, stride):\n",
    "        # Memilih segmen token dengan mempertimbangkan token spesial\n",
    "        window_segment = [tokenizer.cls_token_id] + initial_tokens[i:i + max_length - 2] + [tokenizer.sep_token_id]\n",
    "        window_segments.append(window_segment)\n",
    "\n",
    "    return window_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5bc89ac-f6d7-45e4-a0e5-9b4cebd917b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fungsi untuk menggabungkan token menjadi string\n",
    "def tokens_to_string(tokens):\n",
    "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60f45589-fdad-4149-9745-3326115eb480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Memuat dataset\n",
    "df = pd.read_csv('Merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6c98dce-40d1-4eb9-9afc-8c8d3098c757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pembersihan teks\n",
    "df['cleaned_OCR_text'] = df['OCR_Text'].apply(clean_text)\n",
    "# Sliding window tokenization\n",
    "df['tokenized_segments'] = df['cleaned_OCR_text'].apply(sliding_window_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggabungkan segmen teks yang telah ditokenisasi\n",
    "df['processed_text'] = df['tokenized_segments'].apply(lambda segs: ' '.join([tokens_to_string(seg) for seg in segs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghilangkan stop words\n",
    "def remove_stopwords(text, lang):\n",
    "    words = text.split()\n",
    "    if lang == 'id':\n",
    "        stop_words = set(stopwords.words('indonesian'))\n",
    "    else:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text, lang):\n",
    "    words = text.split()\n",
    "    if lang == 'id':\n",
    "        # Menggunakan Sastrawi untuk bahasa Indonesia\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "    else:\n",
    "        # Menggunakan SnowballStemmer atau PorterStemmer untuk bahasa lain\n",
    "        stemmer = SnowballStemmer(lang) if lang in SnowballStemmer.languages else PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'] = df['processed_text'].apply(lambda x: detect(x) if x.strip() != \"\" else \"\")\n",
    "df['no_stopwords'] = df.apply(lambda x: remove_stopwords(x['processed_text'], x['language']), axis=1)\n",
    "df['stemmed_text'] = df.apply(lambda x: stem_text(x['no_stopwords'], x['language']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
